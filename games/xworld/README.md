# XWorld2D
## <img src="../../doc/xworld2d.png">

## Game description
In XWorld2D, a virtual agent learns language and vision. The agent sees surrounding environment images, listens to a virtual teacher, and takes actions to receive rewards. It should interactively learn the teacherâ€™s language from scratch based on two language use cases: sentence-directed navigation and question answering. It should learn simultaneously the visual representations of the world, the language, and the action control.

## Create
* Python name: ```xworld```
* C++ constructor name: ```XWorldSimulator```
* Name for the unified C++ simulator interface: ```xworld```

## Flags
|**Name**|**Description**|
|:-------|:---------------|
|```pause_screen```|Pause the screen when show_screen() is called, until any key is pressed. (Default: false)|
|```xwd_conf_path```|The JSON file for configuring XWorld2D. (Default: "")|
|```task_mode```|This flag has three possible values (Default: "one_channel") <br> 1. "arxiv_lang_acquisition": replicate the environment used in arXiv:1703.09831 (used with conf file ```<xworld_path>/games/xworld/confs/navigation.json``` and dictionary ```<xworld_path>/games/xworld/dicts/nav_dict.txt```); <br> 2. "arxiv_interactive": replicate the environment used in arXiv:1705.09906 (used with conf file ```<xworld_path>/games/xworld/confs/lang.json``` and dictionary ```<xworld_path>/games/xworld/dicts/lang_dict.txt```); <br> 3. "one_channel": integrate the above two environments into a single one.|
|```context```|How many consecutive frames are used to represent the current sensor input. (Default: 1)|
|```visible_radius```|The visible radius of the agent. If zero, the actual world dimensions of the training image will be (2h-1) x (2w-1). (Default: 0)|

## How to define your own XWorld2D tasks
You can customize XWorld2D tasks in a flexible way. To define a new task, you need to follow three steps:
1. Write a Python class that defines the environment map
  * This class must be defined in a file (the class and file have the same name) and put in
  ```
  <xworld_path>/games/xworld/maps/
  ```
  The class has to inherit from the base class ```XWorldEnv``` (defined in ```xworld_env.py```) and overwrite the member function ```_configure``` to specify how the map is configured. For an example, please take a look at ```XWorldNav.py```.

2. Write a Python class that defines the task
  * This class must be defined in a file (the class and file have the same name) and put in
  ```
  <xworld_path>/games/xworld/tasks/
  ```
  The class has to inherit from the base class ```XWorldTask``` (defined in ```xworld_task.py```). For an example, please take a look at ```XWorldNavTarget.py```.

3. Write a JSON conf file. This file specifies three aspects of the world:
  * ```item_path```: where the icon images are stored. Change this variable if you have new icons.
  * ```map``` : the name of the Python class that defines the map. It should be one of the Python defined maps.
  * ```task_groups``` : how the teacher assigns multiple tasks to the agent. Each task should be one of the Python defined tasks.

  For an example, please take a look at ```<xworld_path>/confs/walls.json```.

## XWorld2D is flexible
With an embedded Python implementation, it is easy for the teacher to dynamically change the environment at every time step, potentially according to the agent's performance and/or behaviors, which is important if you want to implement curriculum learning.

The teacher's sentences can be generated by a context-free grammar (```<xworld_path>/python/context_free_grammar.py```) at each time step of each task. You can define the grammar in an easy way and decide when to generate what sentence in a task.

## Example code for training XWorld2D tasks

Currently there is a PyTorch implementation (by [@zihangdai](https://github.com/zihangdai)) of the model in

* Haonan Yu, Haichao Zhang, Wei Xu, [*A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment*](https://arxiv.org/abs/1703.09831), arXiv:1703.09831, 2017.

The code runs on the whole vocabulary and does not include the zero-shot experiments, but you can choose to do so according to the zero-shot setups discussed in the paper.